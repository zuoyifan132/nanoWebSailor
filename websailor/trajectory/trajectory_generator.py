"""
@File   : agent_difficulty_sampling.py
@Time   : 2025/07/22 10:13
@Author : Evan ZUO
@Desc   : Pipeline for generating and validating agent responses to questions
"""
import os
import re
import sys
import time
import json
import string
import argparse
import jsonlines
import concurrent.futures
from tqdm import tqdm
from pathlib import Path
from loguru import logger
from datetime import datetime
from typing import Callable, Any, List, Dict, Optional, Union, Tuple

# sys.path.append("..")

from websailor.utils.io_utils import read_json_or_jsonl
from websailor.utils import wiki_retrieval_tool
from websailor.trajectory.e2e_agent import E2EAgent

# Get current file information for logging and path management
CUR_FILE_NAME = Path(__file__).resolve().stem
CUR_FILE_DIR = Path(__file__).parent.resolve()

# Constants
DEFAULT_ROLLOUT = 3
DEFAULT_MAX_TURNS = 32
DEFAULT_RETRY_TIMES = 1
RATE_LIMIT_SLEEP = 1  # Sleep time in seconds between API calls

# System prompt template for the end-to-end agent
SYSTEM_PROMPT_FOR_E2E_AGENT_TEMPLATE = """\
You are Alice, an intelligent assistant who specializes in utilizing various tools to help answer user questions.
You MUST follow the Reasoning and Acting (ReAct) Protocol to iteratively make tool calls to achieve the user's purpose.
You should also be aware of the task requirements during ReAct process.

# Reasoning and Acting Protocol
Step 1. Understand the user question and available tools thoroughly, create a plan by breaking down complex questions into manageable steps. Enclose your plan within <plan></plan> XML tags.
Step 2. Think or reflect about your reasoning before using a tool. Enclose your thinking within <think></think> XML tags.
Step 3. Make an appropriate tool call with correct parameters. Enclose your tool call within <tool_call></tool_call> XML tags.
Step 4. Review the tool response that will be enclosed within <tool_response></tool_response> XML tags.
... (Repeat Step 2-4 as needed until you have sufficient information.)
Last Step. When you think all the necessary information has been exhaustively gathered, provide the answer to the user's question within <answer></answer> XML tags.

# Task Requirements
- For complex questions, ensure sufficient tool interaction depth to gather all relevant information.
- If tools cannot solve a problem, clearly explain the limitations and suggest alternatives.\
"""

# Helper Functyion
def check_valid_answer(generated_answer: str, gt_answers: Union[str, List[str]]) -> bool:
    """
    Validate if the generated answer is semantically consistent with the ground truth.
    
    Args:
        generated_answer: The answer generated by the model
        gt_answer: The ground truth answer(s)
        question: The original question
        
    Returns:
        bool: True if the answer is valid, False otherwise
    """
    def normalize_answer(text):
        """
        Standardize text answers, including removing articles, fixing spaces, removing punctuation, and converting to lowercase.
        Args:
            text (str): The text that needs to be standardized
            
        Returns:
            str: The standardized text
        """
        # Remove articles (a, an, the)
        text = re.sub(r"\b(a|an|the)\b", " ", text)
        # Fix spaces - consolidate multiple spaces into one
        text = " ".join(text.split())
        # Remove punctuation
        exclude = set(string.punctuation)
        text = "".join(ch for ch in text if ch not in exclude)
        # Convert to lowercase
        return text.lower()

    
    if isinstance(gt_answers, str):
        gt_answers = [gt_answers]
    
    normalized_generated_answer = normalize_answer(generated_answer)
    
    for gt_answer in gt_answers:
        gt_answer = normalize_answer(gt_answer)
        if gt_answer in normalized_generated_answer:
            return True
    return False

# Helper Functyion
def get_search_tool_description() -> List[Dict[str, Any]]:
    """
    Returns the standard search tool description.
    
    Returns:
        A list containing the search tool description
    """
    tool_desc = [{
        "name": "search",
        "description": "Searches the web for relevant information based on the given query.",
        "parameters": {
            "type": "object",
            "properties": {
                "query_list": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "A list of fully-formed semantic queries. The tool will return search results for each query."
                }
            },
            "required": ["query_list"]
        }
    }]

    return tool_desc 

# Helper Functyion
def validate_file_paths(data_path: str, save_path: str) -> None:
    """
    Validates that the input file exists and the output directory can be created.
    
    Args:
        data_path: Path to the input data file
        save_path: Path to save the output results
        
    Raises:
        FileNotFoundError: If the input file doesn't exist
        PermissionError: If the output directory can't be created
    """
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Input data file not found: {data_path}")
    
    try:
        save_dir = os.path.dirname(save_path)
        os.makedirs(save_dir, exist_ok=True)
    except PermissionError:
        raise PermissionError(f"Cannot create output directory: {os.path.dirname(save_path)}")

# Helper Functyion
def setup_logging(log_dir: Optional[str] = None) -> str:
    """
    Set up logging configuration.
    
    Args:
        log_dir: Optional directory for logs, defaults to standard location
        
    Returns:
        Path to the log file
    """
    # Get current datetime for log file name
    now = datetime.now()
    datetime_str = now.strftime("%Y-%m-%d_%H-%M-%S")
    
    # Determine log directory
    if not log_dir:
        log_dir = os.path.join(CUR_FILE_DIR, "logs", CUR_FILE_NAME)
        
    # Create log directory if it doesn't exist
    os.makedirs(log_dir, exist_ok=True)
    
    # Set up log file path
    log_file = os.path.join(log_dir, f"{datetime_str}.log")
    
    # Configure logger
    logger.add(log_file)
    logger.info(f"Logging to {log_file}")
    
    return log_file

# Helper Functyion
def parse_arguments() -> Dict[str, Any]:
    """
    Parse command line arguments.
    
    Returns:
        Dictionary of configuration values
    """
    parser = argparse.ArgumentParser(
        description="Pipeline for generating and validating agent responses to questions"
    )
    
    parser.add_argument(
        "--data_path", 
        type=str, 
        required=True,
        help="Path to the input data file in JSONL format"
    )
    
    parser.add_argument(
        "--save_path", 
        type=str, 
        default="../../data/temp/void.json",
        help="Path to save the output results in JSONL format"
    )
    
    parser.add_argument(
        "--rollout", 
        type=int, 
        default=DEFAULT_ROLLOUT,
        help=f"Number of retry attempts for each question (default: {DEFAULT_ROLLOUT})"
    )
    
    parser.add_argument(
        "--max_turns", 
        type=int, 
        default=DEFAULT_MAX_TURNS,
        help=f"Maximum conversation turns allowed per session (default: {DEFAULT_MAX_TURNS})"
    )
    
    parser.add_argument(
        "--retry_times", 
        type=int, 
        default=DEFAULT_RETRY_TIMES,
        help=f"Number of retry attempts for API failures (default: {DEFAULT_RETRY_TIMES})"
    )
    
    parser.add_argument(
        "--start_index", 
        type=int, 
        default=0,
        help="Index to start processing from in the dataset (default: 0)"
    )

    parser.add_argument(
        "--max_workers", 
        type=int, 
        default=5,
        help="The maximum of workers to do the concurrent future"
    )
    
    parser.add_argument(
        "--log_dir",
        type=str,
        default=None,
        help="Directory for storing log files (default: ../logs/agent_difficulty_sampling/)"
    )

    parser.add_argument(
        "--test",
        action="store_true",
        help="Enable test mode"
    )
    
    args = parser.parse_args()
    return vars(args)


class Pipeline:
    """
    Pipeline for evaluating agent performance on answering questions using tools.
    Processes data, executes the agent, and saves the results.
    """

    def __init__(
            self, 
            data_path: str, 
            save_path: str, 
            rollout: int = DEFAULT_ROLLOUT, 
            max_turns: int = DEFAULT_MAX_TURNS, 
            retry_times: int = DEFAULT_RETRY_TIMES,
            start_index: int = 0,
            max_workers: int = 5,
            test_mode: bool = False
    ) -> None:
        """
        Initialize the pipeline.
        
        Args:
            data_path: Path to the input data file
            save_path: Path to save the output results
            rollout: Number of runs to execute for each question
            max_turns: Maximum conversation turns allowed
            retry_times: Number of retry attempts for API failures
            start_index: Index to start processing from in the dataset
            max_workers: Maximum number of parallel workers
            test_mode: if enable, only show the result, no save result
        """
        # Validate input parameters
        validate_file_paths(data_path, save_path)
        
        self.data_path = data_path
        self.save_path = save_path
        self.rollout = rollout
        self.max_turns = max_turns
        self.retry_times = retry_times
        self.start_index = start_index
        self.max_workers = max_workers
        self.test_mode = test_mode

        if self.test_mode:
            self.max_turns = DEFAULT_MAX_TURNS
            self.retry_times = DEFAULT_RETRY_TIMES
            self.max_workers = 1
            self.rollout = 1

        system_prompt = SYSTEM_PROMPT_FOR_E2E_AGENT_TEMPLATE

        logger.debug(f"System prompt: {system_prompt}")

        # Initialize the E2E agent
        self.agent = E2EAgent(
            system=system_prompt,
            max_turns=self.max_turns,
            retry_times=self.retry_times,
        )

    def load_data(self) -> List[Dict[str, Any]]:
        """
        Load data from the input file.
        
        Returns:
            List of data items starting from the specified start index
        """
        try:
            data = read_json_or_jsonl(self.data_path)
            logger.info(f"Loaded {len(data)} items from {self.data_path}")
            
            # Apply start index if specified
            if self.start_index > 0:
                if self.start_index >= len(data):
                    logger.warning(f"Start index {self.start_index} exceeds data length {len(data)}")
                    return []
                logger.info(f"Starting from index {self.start_index} out of {len(data)}")
                return data[self.start_index:]
            return data
        except Exception as e:
            logger.error(f"Failed to load data: {str(e)} for {self.data_path}")
            raise

    def prepare_tool_caller(self, tool_desc_list: List[Dict[str, Any]]) -> Dict[str, Callable]:
        """
        Prepare the tool caller dictionary from tool descriptions.
        
        Args:
            tool_desc_list: List of tool descriptions
            
        Returns:
            Dictionary mapping tool names to tool functions
        """
        tool_caller = {}
        for tool_desc in tool_desc_list:
            try:
                tool_caller[tool_desc["name"]] = wiki_retrieval_tool.get_tool(tool_desc["name"])
                if tool_caller[tool_desc["name"]] == None:
                    logger.warning(f"The {tool_desc['name']} tool is None")
            except Exception as e:
                logger.error(f"Failed to get tool {tool_desc['name']}: {str(e)}")
        return tool_caller

    def format_tool_descriptions(self, tool_desc_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Format tool descriptions for the agent.
        
        Args:
            tool_desc_list: Original tool descriptions
            
        Returns:
            Formatted tool descriptions
        """
        return [
            {"type": "function", "function": tool_desc}
            for tool_desc in tool_desc_list
        ]
    
    def execute_single_attempt(self, attempt_id: int, query: str, 
                               formatted_tools: List[Dict[str, Any]],
                               tool_caller: Dict[str, Callable],
                               ground_truth_answer: List[str]) -> Tuple[bool, Optional[List[Dict[str, Any]]]]:
        """
        Execute a single attempt of the agent.
        
        Args:
            attempt_id: Identifier for this attempt
            query: The user question
            formatted_tools: Formatted tool descriptions
            tool_caller: Dictionary mapping tool names to functions
            ground_truth_answer: List of correct answers
            
        Returns:
            Tuple of (success_flag, conversations)
        """
        start_time = time.time()
        if attempt_id == 1:
            logger.info(f"Starting attempt {attempt_id} for query: {query[:50]}...")

        try:
            conversations = self.agent.execute(attempt_id == 1, query, formatted_tools, tool_caller)

            if conversations and conversations[-1]["from"] == "final_answer":
                generated_answer: str = conversations[-1]["value"]

                logger.debug(f"Attempt {attempt_id} - Generated answer: {generated_answer[:100]}...")
                logger.debug(f"Attempt {attempt_id} - Ground truth answer: {ground_truth_answer}")

                pass_flag = check_valid_answer(generated_answer, ground_truth_answer)

                logger.info(f"Attempt {attempt_id} - Answer validity: {pass_flag}")
                
                execution_time = time.time() - start_time
                if attempt_id == 1:
                    logger.info(f"Attempt {attempt_id} completed in {execution_time:.2f} seconds")
                
                return (pass_flag, conversations)
            else:
                if attempt_id == 1:
                    logger.warning(f"Attempt {attempt_id} - No final answer found in conversations")
                return (False, conversations)

        except Exception as e:
            if attempt_id == 1:
                logger.error(f"Attempt {attempt_id} failed with error: {str(e)}")
            return (False, None)

    def execute_agent(self, query: str, tool_desc_list: List[Dict[str, Any]], 
                ground_truth_answer: List[str]) -> List[Tuple[bool, Optional[List[Dict[str, Any]]]]]:
        """
        Execute the agent for rollout attempts in parallel.
        
        Args:
            query: The user question
            tool_desc_list: List of tool descriptions
            ground_truth_answer: List of correct answers
            
        Returns:
            List of (success_flag, conversations) tuples for each attempt, sorted by attempt_id
        """
        # when use claude model need to convert tool description to certain style
        formatted_tools = self.format_tool_descriptions(tool_desc_list)
        tool_caller = self.prepare_tool_caller(tool_desc_list)
        
        results = {}

        # Execute agent attempts in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_attempt = {
                executor.submit(
                    self.execute_single_attempt,
                    i, query, formatted_tools, tool_caller, ground_truth_answer
                ): i
                for i in range(1, self.rollout + 1)
            }

            for future in concurrent.futures.as_completed(future_to_attempt):
                attempt_id = future_to_attempt[future]
                try:
                    result = future.result()
                    results[attempt_id] = result
                except Exception as e:
                    logger.error(f"Attempt {attempt_id} raised an unexpected exception: {str(e)}")
                    results[attempt_id] = (False, None)

        sorted_results = [results[attempt_id] for attempt_id in sorted(results.keys())]
        return sorted_results
    
    def process_item(self, row: Dict[str, Any], idx: int, total: int) -> Optional[Dict[str, Any]]:
        """
        Process a single data item by running the agent multiple times.
        
        Args:
            row: The data item to process
            idx: Current item index
            total: Total number of items
            
        Returns:
            Processed result dictionary with success rate if any attempts were successful
        """
        uid: str = row.get("id", f"unknown-{idx}")
        query: str = row.get("query", "")
        source = row.get("source", "unknown")
        ground_truth_answer = row.get("golden_answers", [])
        
        if not query or not ground_truth_answer:
            logger.warning(f"Skipping item {idx}/{total}: Missing query or ground truth")
            return None
            
        # Get standard search tool description
        tool_desc_list = get_search_tool_description()
        date_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        try:
            # Execute agent multiple times for this query
            attempt_results = self.execute_agent(query, tool_desc_list, ground_truth_answer)
            
            # Calculate success rate
            successful_attempts = sum(1 for success, _ in attempt_results if success)
            success_rate = successful_attempts / len(attempt_results) if attempt_results else 0

            # Use the first successful conversation as the main result
            successful_conversations, non_successful_conversations = None, None
            for success, conversations in attempt_results:
                if success and conversations:
                    successful_conversations = conversations
                    break

            _, last_conversations = attempt_results[-1]
            non_successful_conversations = last_conversations

            if successful_conversations:
                logger.info(f"Processed item {idx}/{total} from source: {source} - Success rate: {success_rate:.2f}")
                
                # Prepare result object
                result = {
                    "id": uid,
                    "submitter": "zyf",
                    "datetime": date_time,
                    "source": source,
                    "task": "agent-wiki",
                    "query": query,
                    "answer": ground_truth_answer,
                    "tools": tool_desc_list,
                    "conversations": successful_conversations,
                    "success_rate": success_rate,
                    "successful_attempts": successful_attempts,
                    "total_attempts": len(attempt_results)
                }
                return result
            else:
                logger.warning(f"No valid conversations generated for item {idx}/{total}")

                # Prepare non-successful result object
                result = {
                    "id": uid,
                    "submitter": "zyf",
                    "datetime": date_time,
                    "source": source,
                    "task": "agent-wiki",
                    "query": query,
                    "answer": ground_truth_answer,
                    "tools": tool_desc_list,
                    "conversations": non_successful_conversations,
                    "success_rate": success_rate,
                    "successful_attempts": successful_attempts,
                    "total_attempts": len(attempt_results)
                }
                return result
                
        except Exception as e:
            logger.error(f"Failed to process item {idx}/{total}: {str(e)}")
            return None
        
    def run(self) -> None:
            """
            Main execution function that processes all data items and saves the results.
            """
            # Load data from input file
            data = self.load_data()
            total = len(data)
            
            if total == 0:
                logger.warning("No data items to process")
                return
            
            if self.test_mode:
                for idx, row in tqdm(enumerate(data, 1), desc="[Task-Data Generation]", total=total):
                    try:
                        # Process the current item
                        result = self.process_item(row, idx, total) 
                    except Exception as e:
                        logger.error(f"Unexpected error processing item {idx}/{total}: {str(e)}")
            else:
                success_count = 0
                
                # Process each data item
                with jsonlines.open(self.save_path, "a") as writer:
                    for idx, row in tqdm(enumerate(data, 1), desc="[Task-Data Generation]", total=total):
                        try:
                            # Process the current item
                            result = self.process_item(row, idx, total)
                            
                            # Save valid results
                            if result:
                                writer.write(result)
                                success_count += 1
                                
                        except Exception as e:
                            logger.error(f"Unexpected error processing item {idx}/{total}: {str(e)}")
                        
                        # Short delay between items to avoid overwhelming resources
                        if idx < total:
                            time.sleep(RATE_LIMIT_SLEEP)  # Reduced sleep time since we're rate-limiting at the attempt level
                            
                success_rate = (success_count / total) * 100 if total > 0 else 0
                logger.info(f"Data generation completed. Processed {success_count}/{total} items successfully ({success_rate:.1f}%)")
                logger.info(f"Output saved to: {self.save_path}")


if __name__ == '__main__':
    """
    Entry point for the script. Sets up logging and runs the pipeline.
    """
    try:        
        # Get configuration
        config = parse_arguments()

        # Set up logging
        log_file = setup_logging(config["log_dir"])
        logger.info(f"Starting with config: {json.dumps(config, indent=2)}")
        
        # Initialize and run the pipeline
        pipeline = Pipeline(
            data_path=config["data_path"],
            save_path=config["save_path"],
            rollout=config["rollout"],
            max_turns=config["max_turns"],
            retry_times=config["retry_times"],
            start_index=config["start_index"],
            max_workers=config["max_workers"],
            test_mode=config["test"]
        )
        
        # Run the pipeline
        pipeline.run()
        
        logger.info("Pipeline execution completed successfully")
        
    except Exception as e:
        logger.exception(f"Pipeline execution failed: {str(e)}")
        sys.exit(1)
